---
aliases:
  - Study - {{title}}
tags:
  - type/study
  - context/studies
  - theme/default
  - status/in-progress
  - review/pending
date: 2025-07-29
last_updated: 2025-07-29
---
#apache #networking 
#  [[Linux Main|Study - {{Linux}}]] 

## OgÃ³lnie
Kluczowym elementem w web development jest komunikacja miÄ™dzy przeglÄ…darkÄ… i serwerem. 
Apache jest silnikiem dla strony.

Albo fundamentami pod dom. Tak jak moÅ¼na dodawaÄ‡ inne pokoje tak do apache moÅ¼na dodawaÄ‡ moduÅ‚y kaÅ¼dy przystosowany do konkretnego uÅ¼ycia.

---
## Content
## Apache 

Instalujemy Apache `sudo apt install apache2 -y`
![[Pasted image 20250729135634.png]]
**NastÄ™pnie moÅ¼emy wystartowaÄ‡ server za pomocÄ…:
- `apache2ctl
- `systemctl
- `service
MoÅ¼na teÅ¼ uÅ¼yÄ‡ apache2 binary ale sie nie zaleca (zmienne Å›rodowiskowe w defaultowej konfiguracji)
`sudo systemctl start apache2`
![[Pasted image 20250729135825.png]]
**Gdy Apache siÄ™ ruchomi moÅ¼emy uÅ¼yÄ‡ przeglÄ…darki do `http://localhost` DomyÅ›lnie na porcie 80
![[apache-default.webp]]
Podczas Ä‡wiczenia na virtualce wyskakuje bÅ‚Ä…d.
Prawdopodobnie bÅ‚Ä…d portu, Å¼eby go zmieniÄ‡
`sudo nano /etc/apache2/port.conf`
Zmieniamy port z 80 na 8080
![[Pasted image 20250729141244.png]]
NastÄ™pnie `sudo systemctl restart apache2`**
Restartujemy Apache i idziemy do `http://localhost:8080` albo uÅ¼ywamy curla.
`curl -I [http://localhost:8080](http://localhost:8080/)`
![[Pasted image 20250729194252.png]]
Korzystanie z narzÄ™dzi CLI, takich jak `curl` i `wget`, pozwala komunikowaÄ‡ siÄ™ z serwerami WWW bez potrzeby uÅ¼ycia przeglÄ…darki. To jak terminalowy odpowiednik przeglÄ…darki â€“ umoÅ¼liwia pobieranie i analizÄ™ zawartoÅ›ci stron internetowych.

Na przykÅ‚ad, prosty skrypt bashowy moÅ¼e pobraÄ‡ stronÄ™ i wyciÄ…gnÄ…Ä‡ z niej wszystkie URL-e. Takie podejÅ›cie przydaje siÄ™ m.in. w web scrapingu, testach automatycznych i monitorowaniu zmian na stronach.

--- 

## cURL
#cURL
cURL to taki dron ktÃ³ry dowozi i odbiera paczki (dane)
ProtokoÅ‚y to rÃ³Å¼ne wysokoÅ›ci na ktÃ³re moÅ¼e siÄ™ wznieÅ›Ä‡

`curl` to uniwersalne narzÄ™dzie CLI do **transferu danych** przez protokoÅ‚y HTTP, HTTPS, FTP, SFTP, FTPS czy SCP. DziÄ™ki niemu moÅ¼esz:

- **PobraÄ‡ plik**: `curl -O https://example.com/file.zip`
- **WysÅ‚aÄ‡ Å¼Ä…danie POST** z danymi JSON: `curl -X POST -H "Content-Type: application/json" -d '{"key":"value"}' https://api.example.com/endpoint`
- **ZobaczyÄ‡ nagÅ‚Ã³wki** odpowiedzi: `curl -I https://example.com`
![[Pasted image 20250729200355.png]]

---
## Wget

To samo co cURl ale zapisuje do pliku jak pajÄ…k z notatnikiem ktÃ³ry mi przynosi notatkÄ™.

![[Pasted image 20250729200602.png]]

---
## Python 3

Wielka KrÃ³lowa pajÄ…kÃ³w, spawnuje chmarÄ™ pajÄ…czkÃ³w (skryptÃ³w) i sam mogÄ™ zdecydowaÄ‡ co robiÄ….
- KaÅ¼dy to moduÅ‚: `requests` to dronâ€‘kurier webowy, `socket` to surowy straÅ¼nik sieci, `os` to zaklinacz systemu plikÃ³w.
- ğŸ¤– **Programuje ich zachowanie**  
	Piszesz skrypt, definiujesz trasÄ™, metody zbierania danych, filtry, reguÅ‚y i reakcje (np. gdy status = 500, wyÅ›lij alert emailem).
## PrzykÅ‚ad â€pajÄ™czego szkieletuâ€ w Pythonie

```import requests, time

def spider_flight(url):
    r = requests.get(url)
    print(f"[{time.strftime('%H:%M:%S')}] {url} â†’ {r.status_code}")

if __name__ == "__main__":
    targets = ["https://example.com", "https://api.service/health"]
    for t in targets:
        spider_flight(t)
        time.sleep(1)
```


## Review Questions
- Question 1?
- Question 2?

## Summary
Write a summary of what you've learned.

## References
- [Reference 1](link)
- [Reference 2](link)